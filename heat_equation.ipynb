{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Physics-Informed Neural Network to solve the 1d heat equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will solve the one-dimensional heat equation using a physics-informed neural network (PINN) implemented in TensorFlow. The problem definition can be interpreted as **cooling a one-dimensional rod** from an initial temperature distribution $u(x,0)=f(x)$. The governing partial differential equation (PDE) with the corresponding initial (IC) and boundary (BC) condition are given in **non-dimensionalized form** by:\n",
    "\n",
    "* PDE: $\\frac{\\partial u(x,t)}{\\partial t} - \\frac{\\partial^2 u(x,t)}{\\partial x^2}=0$,\n",
    "*  IC: $u(x,0)=f(x), \\quad\\quad\\quad\\quad 0<x<1$, \n",
    "*  BC: $u(0,t)=u(1,t)=0, \\quad\\quad    t>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Physics-Informed Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed by approximating the solution function $u(x,t)$ with a fully-connected neural network function $u_\\theta(x,t)$. The input-dimension will be set to two ($x,t$), the output dimension to one ($u$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neural_network import NeuralNetwork\n",
    "PINN = NeuralNetwork(n_hidden=4, n_neurons=50, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will construct the PDE (definition given above) and define the **Physics Loss** $L_F$. We can use the Tensorflow [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) function to obtain the partial derivatives $\\frac{\\partial u_\\theta}{\\partial t}$ and $\\frac{\\partial^2 u_\\theta}{\\partial x^2}$ with automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_F(X_col):\n",
    "   \n",
    "    # tape forward propergation to retrieve gradients\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(X_col)\n",
    "        with tf.GradientTape() as tt:\n",
    "            tt.watch(X_col)\n",
    "            U = PINN(X_col)\n",
    "        U_d = tt.batch_jacobian(U, X_col)        \n",
    "    U_dd = t.batch_jacobian(U_d, X_col)\n",
    "\n",
    "    # U_d shape: (n_col, u_i, dx_i)\n",
    "    u_t = U_d[:, 0, 1]\n",
    "    # U_dd shape: (n_col, u_i, dx_i, dx_j)\n",
    "    u_xx = U_dd[:, 0, 0, 0]\n",
    "\n",
    "    # Heat diffusion equation\n",
    "    res_F = u_t - u_xx\n",
    "    \n",
    "    # determine residual errors \n",
    "    loss_F = tf.reduce_mean(tf.square(res_F))\n",
    "\n",
    "    return loss_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **Data Loss** we simply use the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_u(X, u):\n",
    "    u_pred = PINN(X)     \n",
    "    return tf.reduce_mean(tf.square(u_pred - u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the training step. To define the **multi-objective loss** we use the (unweighted) **linear combination of both losses** - motivated persons could play around with weighting both losses differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(optimizer, X, u, X_col):\n",
    "    \n",
    "    # open a GradientTape to record forward/loss pass                   \n",
    "    with tf.GradientTape() as tape:  \n",
    "        loss_u = get_loss_u(X, u)\n",
    "        loss_F = get_loss_F(X_col)\n",
    "                \n",
    "        # Linear combination of data and physics loss\n",
    "        loss = loss_u + loss_F\n",
    "\n",
    "\n",
    "    # gradients PDE fit\n",
    "    grads = tape.gradient(loss, PINN.trainable_weights)                    \n",
    "    # perform single GD step \n",
    "    optimizer.apply_gradients(zip(grads, PINN.trainable_weights))\n",
    "    \n",
    "    return loss_u, loss_F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets for IC, BC and collocation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the initial temperature distribution $f(x)$ for the **initial condition** (IC). To measure the model prediction against a test set, we use $f(x)=sin(\\pi x)$ as an initial temperature distribution (since the solution for this problem is given analytically and will be implemented below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.sin(np.pi*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an input dataset $\\{x^{(i)},0\\}_{i=1}^{N_{IC}}$ with the corresponding temperature values and convert it to a tensorflow tensor. **Note: $x\\in[0,1]$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IC = 100\n",
    "### Sample data from IC ###\n",
    "x_IC = np.random.rand(N_IC)\n",
    "u_IC = f(x_IC)\n",
    "\n",
    "# Input data has to be converted to tf.tensors\n",
    "X_IC = tf.convert_to_tensor([[x, 0] for x in x_IC], dtype=tf.float32)\n",
    "u_IC = tf.convert_to_tensor([[u] for u in u_IC], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **boundary condition** (BC) we sample time values and construct the input datasets $\\{0, t^{(i)}\\}_{i=1}^{N_{BC}}$ and $\\{1, t^{(i)}\\}_{i=1}^{N_{BC}}$. Here, we **limit the computational domain** of the time coordinate to $t\\in[0,0.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BC = 100\n",
    "### Sample data from BC ###\n",
    "X_bot = [[0, t] for t in np.random.rand(N_BC)*0.5]\n",
    "X_top = [[1, t] for t in np.random.rand(N_BC)*0.5]\n",
    "u_bot = [[0] for _ in range(N_BC)]\n",
    "u_top = [[0] for _ in range(N_BC)]\n",
    "\n",
    "# Input data has again to be converted to tf.tensors\n",
    "X_BC = tf.convert_to_tensor(X_bot + X_top, dtype=tf.float32)\n",
    "u_BC = tf.convert_to_tensor(u_bot + u_top, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we **plot** the IC and BC datasets and concatenate them to a **single training dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots import plot_input_data\n",
    "plot_input_data(X_IC, u_IC, X_BC, u_BC)\n",
    "\n",
    "# Concatenate to single training data set\n",
    "X = tf.concat([X_IC, X_BC], axis=0)\n",
    "u = tf.concat([u_IC, u_BC], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next, important step, we sample the **collocation points** from inside the function domain using **latin-hypercube sampling** found in the pyDOE module. The sampled domain will be $x,t\\in[0,1]\\times[0,0.5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDOE import lhs\n",
    "N_col = 2500\n",
    "### Sample collocation points ###\n",
    "X_col = tf.convert_to_tensor([1, 0.5] * lhs(2, N_col), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to test our model against a test set, we define the analytical solution for our problem definition which is given by $u(x,t)=sin(\\pi x)\\cdot exp(-\\pi^2 t)$. (**CAUTION**: the analytical solution will we only valid for $f(x)=sin(\\pi x)$ and $u(0,t)=u(1,t)=1$ with $t>0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_u_solution(X):\n",
    "    x = X[:,0]\n",
    "    t = X[:,1]\n",
    "    u_sol = np.sin(np.pi*x) * np.exp(-np.pi**2*t)\n",
    "    return tf.expand_dims(u_sol, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample test points inside the function domain and get the corresponding temperature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 1000\n",
    "### Sample collocation points ###\n",
    "X_test = tf.convert_to_tensor([1, 0.5] * lhs(2, N_test), dtype=tf.float32)\n",
    "u_test = get_u_solution(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN Training (PDE Solving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step we have to do, is implementing the training loop which calls the *train_step()* function.\n",
    "Here, we use the Adam optimizer in full-batch mode. The initial learning rate is set to $1e-3$ which should be ok for this basic test. Learning rate schedule, e.g. exponential decay, could be used to further optimize and finetune the model training. Number of epochs can be changed at will. (**Note**: for an enormous run time speed up, we can use [tf.function()](https://www.tensorflow.org/api_docs/python/tf/function) on the *train_step()* function which converts it into a Tensorflow graph function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-03\n",
    "\n",
    "# Adam optimizer with default settings for momentum\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)  \n",
    "# convert function to tf.function (graph function)\n",
    "train_step_tf = tf.function(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executing the following cell will start the PINN training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    \n",
    "    loss_u, loss_F = train_step_tf(optimizer, X, u, X_col)\n",
    "    \n",
    "    if (epoch % 100 == 0):\n",
    "        print(f'Epoch {epoch:<5} || Loss_u: {loss_u:1.2e} | Loss_F: {loss_F:1.2e}')\n",
    "        \n",
    "print(\"### Finished training ###\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how well the model was doing in solving the PDE, we plot the model prediction over the entire function domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots import plot_prediction\n",
    "plot_prediction(PINN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have used the *default* problem definition, we can also check the model's accucary by using the test set. A common performance measure is the **relative L2 Error** given by $rel. L^2 = ||u_{pred}-u_{true}||/||u_{true}||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN(X_test)\n",
    "rel_L2 = np.linalg.norm(u_pred-u_test)/np.linalg.norm(u_test)\n",
    "print(f\"rel. L2 Error: {rel_L2*100:1.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

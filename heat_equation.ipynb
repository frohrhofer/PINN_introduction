{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Physics-Informed Neural Network to Solve the 1D Heat Equation\n",
    "\n",
    "In this simple tutorial, we will solve the one-dimensional heat equation using a Physics-Informed Neural Network (PINN) implemented in TensorFlow. The problem definition can be interpreted as **cooling a one-dimensional rod** from an initial temperature distribution $u(x,0)=f(x)$. \n",
    "\n",
    "The problem will be treated as a **forward problem**, i.e. we will define the initial condition (IC) and boundary conditions (BC), as well as the governing differential equation, to approximate the (unique) solution function $u(x,t)$ using a PINN.\n",
    "\n",
    "The governing partial differential equation (PDE) with the corresponding initial (IC) and boundary (BC) condition are given in non-dimensionalized form by:\n",
    "\n",
    "- **PDE:** $\\frac{\\partial u(x,t)}{\\partial t} - \\frac{\\partial^2 u(x,t)}{\\partial x^2} = 0$\n",
    "- **IC:** $u(x,0) = f(x), \\quad 0 < x < 1$\n",
    "- **BC:** $u(0,t) = u(1,t) = 0, \\quad t > 0$\n",
    "\n",
    "Let's start by loading the necessary modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Initial Condition and Analytical Solution\n",
    "\n",
    "For this tutorial, we use $f(x)=sin(\\pi x)$ as an initial temperature distribution, since the solution for this problem is given analytically $\\left(u(x,t)=sin(\\pi x)\\cdot exp(-\\pi^2 t)\\right)$ and we can thus easily evaluate the performance of the PINN afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the initial temperate distribution\n",
    "f = lambda x: np.sin(np.pi*x)\n",
    "\n",
    "# Analytical solution\n",
    "def get_solution(X):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution (only valid for this particular problem setup).\n",
    "\n",
    "    Parameters:\n",
    "    - X: An array of spatio-temporal coordinates (x,t).\n",
    "\n",
    "    Returns:\n",
    "    - u_sol: The analytical solution for the temperature at the specified coordinates.\n",
    "    \"\"\"\n",
    "    x = X[:,0]\n",
    "    t = X[:,1]\n",
    "    u_solution = f(x) * np.exp(-np.pi**2*t)\n",
    "    # Expand to 2d-array (needed for PINN training)\n",
    "    return np.expand_dims(u_solution, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss Functions\n",
    "\n",
    "Next, we construct the **Physics Loss** $L_F$ that encodes the PDE, as well as the **Data Loss** $L_D$ that will enforce the IC and BC. For the physics loss, we use TensorFlow's `tf.GradientTape()` to obtain the partial derivatives $\\frac{\\partial u_\\theta}{\\partial t}$ and $\\frac{\\partial^2 u_\\theta}{\\partial x^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(model, X_collocation):\n",
    "    \"\"\"\n",
    "    Compute the physics-based loss for the PINN model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The PINN model which approximates the solution u(x,t).\n",
    "    - X_col: A tensor containing the collocation points (x, t) where the PDE is enforced.\n",
    "\n",
    "    Returns:\n",
    "    - loss_physics: The mean squared residual of the PDE at the collocation points.\n",
    "    \"\"\"\n",
    "    # First GradientTape for first-order derivative\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch(X_collocation)\n",
    "        # Second GradientTape for second-order derivative\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch(X_collocation)\n",
    "            u_pred = model(X_collocation)\n",
    "        # First derivative w.r.t position\n",
    "        u_x = tape2.gradient(u_pred, X_collocation)[:, 0]\n",
    "    # First derivative w.r.t. time\n",
    "    u_t = tape1.gradient(u_pred, X_collocation)[:, 1]\n",
    "    # Second derivative w.r.t. position\n",
    "    u_xx = tape1.gradient(u_x, X_collocation)[:, 0]\n",
    "\n",
    "    # Physics residuals as given by the differential equation\n",
    "    f_res = u_t - u_xx\n",
    "    # Mean squared error loss for the physics residuals\n",
    "    return tf.reduce_mean(tf.square(f_res))\n",
    "\n",
    "def data_loss(model, X_train, u_train):\n",
    "    \"\"\"\n",
    "    Compute the data loss for the PINN model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The PINN model which approximates the solution u(x,t).\n",
    "    - X_train: A tensor containing the initial/boundary points (x, t).\n",
    "    - u_train: A tensor containing the known values of u at the initial/boundary points.\n",
    "\n",
    "    Returns:\n",
    "    - loss_data: The mean squared error between the predicted and known values of u at the initial/boundary points.\n",
    "    \"\"\"\n",
    "    u_pred = model(X_train)\n",
    "    # Mean squared error loss for the data residuals\n",
    "    return tf.reduce_mean(tf.square(u_pred - u_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Sampling Functions for Data Points\n",
    "\n",
    "To train the PINN, we need to create datasets for the IC, BC, and collocation points where the PDE is enforced. We generate these points using the given problem definition (see above).\n",
    "\n",
    "We implement an auxilary function, that randomly samples data points from the **computational domain $x\\in[0,1]$ and $t\\in[0,0.5]$**. This function will be later on used to sample collocation points for the physics loss function, as well as datasets for the test set and IC/BC (that will be further specified in the later part). The standard sampling approach for PINN is **Latin hypercube sampling** (lhs) to generate a near-random sample from a multidimensional distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDOE import lhs\n",
    "\n",
    "def sample_domain(n_sample):\n",
    "    \"\"\"\n",
    "    Generate random data sampled from the computational domain.\n",
    "\n",
    "    Parameter:\n",
    "    - n_sample: An integer defining the number of data points sampled from the computational domain\n",
    "\n",
    "    Returns:\n",
    "    - X: The coordinates of the sampled data points\n",
    "    \"\"\"\n",
    "    return [1, 0.5] * lhs(2, n_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a random input dataset $\\{x^{(i)},0\\}_{i=1}^{N_{IC}}$ for the IC, with the corresponding temperature values. **Note: $x\\in[0,1]$ while $t=0$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_IC(n_IC):\n",
    "    \"\"\"\n",
    "    Generate random sample of the IC points (x, 0) and their corresponding temperature values\n",
    "\n",
    "    Parameter:\n",
    "    - n_IC: An integer defining the number of data points sampled from the initial conditions\n",
    "\n",
    "    Returns:\n",
    "    - X_IC: The coordinates of the initial conditions points\n",
    "    - u_IC: The temperature values for the initial conditions points\n",
    "    \"\"\"\n",
    "    X_IC = sample_domain(n_IC)\n",
    "    # Set time to zero (x, 0)\n",
    "    X_IC[:, 1] = 0\n",
    "    # Get temperature from the initial condition (expand to 2d-array)\n",
    "    u_IC = np.expand_dims(f(X_IC[:, 0]), axis=1)\n",
    "    return X_IC, u_IC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the BC, we randomly sample time values and construct the input datasets $\\{0, t^{(i)}\\}_{i=1}^{N_{BC}}$ and $\\{1, t^{(i)}\\}_{i=1}^{N_{BC}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_BC(n_BC):\n",
    "    \"\"\"\n",
    "    Generate random sample of the IC points (x, 0) and their corresponding temperature values\n",
    "\n",
    "    Parameter:\n",
    "    - n_BC: An integer defining the number of data points sampled from the boundary conditions at each boundary (left and right).\n",
    "\n",
    "    Returns:\n",
    "    - X_BC: The coordinates of the boundary conditions points\n",
    "    - u_BC: The temperature values for the boundary conditions points\n",
    "    \"\"\"\n",
    "    # left boundary (0, t)\n",
    "    X_left = sample_domain(n_BC)\n",
    "    X_left[:, 0] = 0\n",
    "    # right boundary (1, t)\n",
    "    X_right = sample_domain(n_BC)\n",
    "    X_right[:, 0] = 1\n",
    "    \n",
    "    # Set temperature to zero (cooling)\n",
    "    u_left = np.zeros((n_BC, 1))\n",
    "    u_right = np.zeros((n_BC, 1))\n",
    "\n",
    "    # Combine left and right boundaries to a single dataset\n",
    "    X_BC = np.vstack([X_left, X_right])\n",
    "    u_BC = np.vstack([u_left, u_right])\n",
    "    return X_BC, u_BC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and Preprocessing Datasets\n",
    "\n",
    "Since we now have implemented all necessary sampling functions for the IC, BC, and collocation points, we now proceed by sampling and plotting the datasets. **Note:** The collocation points do not have any label attached to them (that is why we call the training semi-supervised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots import plot_datasets\n",
    "\n",
    "# Specifying number of sampled data points\n",
    "n_IC = 100\n",
    "n_BC = 100\n",
    "n_collocation = 1000\n",
    "\n",
    "# Generate datasets using sampling functions\n",
    "X_IC, u_IC = sample_IC(n_IC)\n",
    "X_BC, u_BC = sample_BC(n_BC)\n",
    "X_collocation = sample_domain(n_collocation)\n",
    "\n",
    "# Plot the sampled data points\n",
    "plot_datasets(X_IC, u_IC, X_BC, u_BC, X_collocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final preprocessing step, we combine the dataset for IC and BC into a single dataset used for the data loss function. Additionally, we convert all dataset from a `np.array` to `tf.tensor`, necessary for training the PINN using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate to single training data set\n",
    "X_train = np.concatenate([X_IC, X_BC], axis=0)\n",
    "u_train = np.concatenate([u_IC, u_BC], axis=0)\n",
    "\n",
    "# Convert to tf.Tensor\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "u_train = tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "X_collocation = tf.convert_to_tensor(X_collocation, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Function and Optimizer\n",
    "\n",
    "We now implement the `train_step()` function that is used to update the model parameters based on the achieved losses. To obtain the derivative of the loss function with respect to the model parameters, we again use the `tf.GradientTape()` function as implemented in Tensorflow.\n",
    "\n",
    "One important aspect of PINN training will be used in the function: We have two losses - the data loss $L_D$ encoding the IC/BC, and the physics loss $L_F$ encoding the PDE. Both losses are considered in the PINN training as **Multi-Objective**, and the standard approach for PINN training is formulating a single loss function by the (unweighted) linear combination of both: $L=L_D+L_F$.\n",
    " \n",
    "\n",
    "(**Note:** For a significant runtime speedup, we can use `@tf.function` decorator on the `train_step()` function to convert it into a TensorFlow graph function.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(optimizer, model, X_train, u_train, X_collocation):\n",
    "    \"\"\"\n",
    "    Perform one training step for the PINN model.\n",
    "\n",
    "    Parameters:\n",
    "    - optimizer: The optimizer used to minimize the loss.\n",
    "    - model: The PINN model which approximates the solution u(x,t).\n",
    "    - X_train: A tensor containing the IC/BC points (x, t).\n",
    "    - u_train: A tensor containing the known values of u at the IC/BC points.\n",
    "    - X_collocation: A tensor containing the collocation points (x, t) where the PDE is enforced.\n",
    "\n",
    "    Returns:\n",
    "    - loss_u: The loss value for the data loss\n",
    "    - loss_F: The loss value for the physics loss\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Data loss\n",
    "        loss_data = data_loss(model, X_train, u_train)\n",
    "        # Physic loss\n",
    "        loss_physics = physics_loss(model, X_collocation)\n",
    "\n",
    "        # Linear combination of data and physics loss\n",
    "        loss_total = loss_data + loss_physics\n",
    "    # Retrieve derivative of the total loss function w.r.t the model parameters\n",
    "    grads = tape.gradient(loss_total, model.trainable_variables)\n",
    "    # Perform a single update step\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_data, loss_physics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually apply the changes to the model parameters in the function implemented above, we have to specify a suitable optimizer. In this case (and as standard for training neural networks) we will use Adam with a default learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate and initialize the optimizer\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Physics-Informed Neural Network\n",
    "\n",
    "We are close to start our model training. The only component missing is the model itself!\n",
    "\n",
    "To approximate the solution function $u(x,t)$, we use a fully-connected neural network as our model. This model represents a continous function approximator $u_\\theta(x,t)$, where $\\theta$ represents the trainable model parameters. The input dimension will be automatically set to two (for the input coordinates $x$ and $t$), and the output dimension will be set to one (for the target value $u$). The number of hidden layers, neurons per layer and activation function can be changed at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neural_network import NeuralNetwork\n",
    "\n",
    "# Initialize the neural network\n",
    "PINN = NeuralNetwork(n_hidden=2, n_neurons=20, activation='tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing PINN Training\n",
    "\n",
    "Now we finally start the PINN training with a specified number of training epochs. Within the training loop, a single training step is performed by calling the previously implemented `train_step()` function. Additionally, we use some logging and printing to better understand whether we are effectively training the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# Number of training epochs\n",
    "n_epochs = 10000\n",
    "# Create empty list for recording training logs\n",
    "log_data, log_physics = [], []\n",
    "# Start training loop\n",
    "start_time = time()\n",
    "for epoch in range(n_epochs):\n",
    "    # Perform a single training step\n",
    "    loss_data, loss_physics = train_step(optimizer, PINN, X_train, u_train, X_collocation)\n",
    "    # Write loss values to log\n",
    "    log_data.append(float(loss_data))\n",
    "    log_physics.append(float(loss_physics))\n",
    "    # Print the loss values \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch:<5} || Loss_data: {loss_data:1.2e} || Loss_physics: {loss_physics:1.2e}')\n",
    "\n",
    "print(f\"===== Finished training in {time()-start_time:.1f} seconds =====\")\n",
    "print(f'Final Value || Loss_data: {loss_data:1.2e} || Loss_physics: {loss_physics:1.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Since we have now trained our PINN, we can perform a series of model evaluation. We start by looking at the **learning curves** that show the history of loss values plotted against the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots import plot_learning_curves\n",
    "plot_learning_curves(log_data, log_physics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the trained model to make predictions on the solution function over the entire computational domain, which we tried to approximate with our PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plots import plot_prediction\n",
    "plot_prediction(PINN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also check the model's accucary by using a dedicated test set. A common performance measure is the **relative L2 Error** given by $rel. L^2 = ||u_{pred}-u_{true}||/||u_{true}||$. For the test set, we just use a random sample of data points within the computational domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a random test dataset\n",
    "X_test = sample_domain(n_sample=1000)\n",
    "u_test = get_solution(X_test)\n",
    "\n",
    "# Make predictions\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "u_pred = PINN(X_test)\n",
    "\n",
    "# Determine the relative L2 Error\n",
    "rel_L2 = np.linalg.norm(u_pred-u_test)/np.linalg.norm(u_test)\n",
    "print(f\"rel. L2 Error: {rel_L2*100:1.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
